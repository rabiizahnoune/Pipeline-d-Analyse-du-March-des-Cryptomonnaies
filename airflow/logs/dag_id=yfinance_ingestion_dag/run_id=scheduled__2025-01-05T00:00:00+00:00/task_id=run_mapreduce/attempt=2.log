[2025-02-26T20:24:21.458+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:24:21.692+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:24:21.698+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-02-26T20:24:21.875+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_mapreduce> on 2025-01-05 00:00:00+00:00
[2025-02-26T20:24:21.937+0000] {standard_task_runner.py:57} INFO - Started process 1031 to run task
[2025-02-26T20:24:21.964+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'yfinance_ingestion_dag', 'run_mapreduce', 'scheduled__2025-01-05T00:00:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpdsil9dcf']
[2025-02-26T20:24:21.981+0000] {standard_task_runner.py:85} INFO - Job 113: Subtask run_mapreduce
[2025-02-26T20:24:22.349+0000] {task_command.py:415} INFO - Running <TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [running]> on host c96a1011f5a3
[2025-02-26T20:24:22.870+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='etudiant' AIRFLOW_CTX_DAG_ID='yfinance_ingestion_dag' AIRFLOW_CTX_TASK_ID='run_mapreduce' AIRFLOW_CTX_EXECUTION_DATE='2025-01-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-01-05T00:00:00+00:00'
[2025-02-26T20:24:22.876+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-02-26T20:24:22.881+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        docker exec -i -u root namenode bash -c \'         cd /tmp &&         cp /mnt/hadoop_data/mapreduce/mapper.py . &&         cp /mnt/hadoop_data/mapreduce/reducer.py . &&         chmod +x mapper.py reducer.py &&         hdfs dfs -rm -r hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05 || true &&         hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar         -files mapper.py,reducer.py         -mapper "python mapper.py"         -reducer "python reducer.py"         -input hdfs:///user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json         -output hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05\'\n        ']
[2025-02-26T20:24:22.960+0000] {subprocess.py:86} INFO - Output:
[2025-02-26T20:24:39.827+0000] {subprocess.py:93} INFO - rm: `hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05': No such file or directory
[2025-02-26T20:24:53.098+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:53,095 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[2025-02-26T20:24:53.764+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:53,763 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-02-26T20:24:53.766+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:53,763 INFO impl.MetricsSystemImpl: JobTracker metrics system started
[2025-02-26T20:24:53.852+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:53,850 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
[2025-02-26T20:24:55.445+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:55,443 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop/mapred/staging/root227759297/.staging/job_local227759297_0001
[2025-02-26T20:24:55.448+0000] {subprocess.py:93} INFO - 2025-02-26 20:24:55,446 ERROR streaming.StreamJob: Error Launching job : Input path does not exist: hdfs:/user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json
[2025-02-26T20:24:55.449+0000] {subprocess.py:93} INFO - Streaming Command Failed!
[2025-02-26T20:24:55.104+0000] {subprocess.py:97} INFO - Command exited with return code 5
[2025-02-26T20:24:55.172+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 5.
[2025-02-26T20:24:55.192+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=yfinance_ingestion_dag, task_id=run_mapreduce, execution_date=20250105T000000, start_date=20250226T202421, end_date=20250226T202455
[2025-02-26T20:24:55.244+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 113 for task run_mapreduce (Bash command failed. The command returned a non-zero exit code 5.; 1031)
[2025-02-26T20:24:55.328+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-26T20:24:55.575+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-26T20:37:25.711+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:37:25.775+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:37:25.806+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-02-26T20:37:25.894+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_mapreduce> on 2025-01-05 00:00:00+00:00
[2025-02-26T20:37:25.916+0000] {standard_task_runner.py:57} INFO - Started process 235 to run task
[2025-02-26T20:37:25.941+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'yfinance_ingestion_dag', 'run_mapreduce', 'scheduled__2025-01-05T00:00:00+00:00', '--job-id', '209', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpbcgyjsvq']
[2025-02-26T20:37:25.961+0000] {standard_task_runner.py:85} INFO - Job 209: Subtask run_mapreduce
[2025-02-26T20:37:26.157+0000] {task_command.py:415} INFO - Running <TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [running]> on host 3c0faf52b292
[2025-02-26T20:37:26.603+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='etudiant' AIRFLOW_CTX_DAG_ID='yfinance_ingestion_dag' AIRFLOW_CTX_TASK_ID='run_mapreduce' AIRFLOW_CTX_EXECUTION_DATE='2025-01-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-01-05T00:00:00+00:00'
[2025-02-26T20:37:26.626+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-02-26T20:37:26.632+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        docker exec -i -u root namenode bash -c \'         cd /tmp &&         cp /mnt/hadoop_data/mapreduce/mapper.py . &&         cp /mnt/hadoop_data/mapreduce/reducer.py . &&         chmod +x mapper.py reducer.py &&         hdfs dfs -rm -r hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05 || true &&         hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar         -files mapper.py,reducer.py         -mapper "python mapper.py"         -reducer "python reducer.py"         -input hdfs:///user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json         -output hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05\'\n        ']
[2025-02-26T20:37:26.699+0000] {subprocess.py:86} INFO - Output:
[2025-02-26T20:37:44.566+0000] {subprocess.py:93} INFO - rm: `hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05': No such file or directory
[2025-02-26T20:38:03.815+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:03,759 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[2025-02-26T20:38:04.853+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:04,835 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-02-26T20:38:04.857+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:04,835 INFO impl.MetricsSystemImpl: JobTracker metrics system started
[2025-02-26T20:38:05.022+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:05,019 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
[2025-02-26T20:38:09.186+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:09,166 INFO mapred.FileInputFormat: Total input files to process : 1
[2025-02-26T20:38:10.510+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:10,507 INFO mapreduce.JobSubmitter: number of splits:1
[2025-02-26T20:38:13.809+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:13,802 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1354639377_0001
[2025-02-26T20:38:13.821+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:13,802 INFO mapreduce.JobSubmitter: Executing with tokens: []
[2025-02-26T20:38:15.829+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:15,827 INFO mapred.LocalDistributedCacheManager: Localized file:/tmp/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1354639377_0001_96a9eca9-8cf8-4dfc-9475-c3288fa27f2f/mapper.py
[2025-02-26T20:38:16.176+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:16,169 INFO mapred.LocalDistributedCacheManager: Localized file:/tmp/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1354639377_0001_f2036849-9284-4dec-86ff-d696d9300fcc/reducer.py
[2025-02-26T20:38:17.411+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,400 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
[2025-02-26T20:38:17.451+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,439 INFO mapreduce.Job: Running job: job_local1354639377_0001
[2025-02-26T20:38:17.511+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,474 INFO mapred.LocalJobRunner: OutputCommitter set in config null
[2025-02-26T20:38:17.563+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,551 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[2025-02-26T20:38:17.653+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,649 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
[2025-02-26T20:38:17.659+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:17,654 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-02-26T20:38:18.480+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:18,468 INFO mapreduce.Job: Job job_local1354639377_0001 running in uber mode : false
[2025-02-26T20:38:18.510+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:18,481 INFO mapreduce.Job:  map 0% reduce 0%
[2025-02-26T20:38:19.140+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,135 INFO mapred.LocalJobRunner: Waiting for map tasks
[2025-02-26T20:38:19.307+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,285 INFO mapred.LocalJobRunner: Starting task: attempt_local1354639377_0001_m_000000_0
[2025-02-26T20:38:19.726+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,724 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
[2025-02-26T20:38:19.727+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,724 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-02-26T20:38:19.897+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,895 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
[2025-02-26T20:38:19.942+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:19,941 INFO mapred.MapTask: Processing split: hdfs://namenode:9000/user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json:0+19757
[2025-02-26T20:38:20.288+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:20,282 INFO mapred.MapTask: numReduceTasks: 1
[2025-02-26T20:38:21.688+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:21,586 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
[2025-02-26T20:38:21.697+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:21,687 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
[2025-02-26T20:38:21.702+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:21,690 INFO mapred.MapTask: soft limit at 83886080
[2025-02-26T20:38:21.705+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:21,690 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
[2025-02-26T20:38:21.708+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:21,691 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
[2025-02-26T20:38:22.376+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:22,360 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[2025-02-26T20:38:22.962+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:22,955 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/python, mapper.py]
[2025-02-26T20:38:23.068+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,019 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir
[2025-02-26T20:38:23.069+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,021 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start
[2025-02-26T20:38:23.070+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,034 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
[2025-02-26T20:38:23.080+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,035 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
[2025-02-26T20:38:23.101+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,054 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
[2025-02-26T20:38:23.103+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,055 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir
[2025-02-26T20:38:23.103+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,056 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file
[2025-02-26T20:38:23.104+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,057 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
[2025-02-26T20:38:23.105+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,060 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length
[2025-02-26T20:38:23.106+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,078 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
[2025-02-26T20:38:23.106+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,078 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name
[2025-02-26T20:38:23.108+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:23,094 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
[2025-02-26T20:38:24.691+0000] {subprocess.py:93} INFO - 2025-02-26 20:38:24,688 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[2025-02-26T20:38:45.324+0000] {subprocess.py:97} INFO - Command exited with return code 137
[2025-02-26T20:38:45.532+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 137.
[2025-02-26T20:38:45.566+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=yfinance_ingestion_dag, task_id=run_mapreduce, execution_date=20250105T000000, start_date=20250226T203725, end_date=20250226T203845
[2025-02-26T20:38:45.732+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 209 for task run_mapreduce (Bash command failed. The command returned a non-zero exit code 137.; 235)
[2025-02-26T20:38:45.805+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-26T20:38:45.935+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-26T20:45:58.915+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:45:58.950+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [queued]>
[2025-02-26T20:45:58.956+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-02-26T20:45:59.035+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_mapreduce> on 2025-01-05 00:00:00+00:00
[2025-02-26T20:45:59.048+0000] {standard_task_runner.py:57} INFO - Started process 2754 to run task
[2025-02-26T20:45:59.063+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'yfinance_ingestion_dag', 'run_mapreduce', 'scheduled__2025-01-05T00:00:00+00:00', '--job-id', '394', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpzrw9ous3']
[2025-02-26T20:45:59.069+0000] {standard_task_runner.py:85} INFO - Job 394: Subtask run_mapreduce
[2025-02-26T20:45:59.203+0000] {task_command.py:415} INFO - Running <TaskInstance: yfinance_ingestion_dag.run_mapreduce scheduled__2025-01-05T00:00:00+00:00 [running]> on host 3c0faf52b292
[2025-02-26T20:45:59.525+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='etudiant' AIRFLOW_CTX_DAG_ID='yfinance_ingestion_dag' AIRFLOW_CTX_TASK_ID='run_mapreduce' AIRFLOW_CTX_EXECUTION_DATE='2025-01-05T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-01-05T00:00:00+00:00'
[2025-02-26T20:45:59.534+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-02-26T20:45:59.547+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        docker exec -i -u root namenode bash -c \'         cd /tmp &&         cp /mnt/hadoop_data/mapreduce/mapper.py . &&         cp /mnt/hadoop_data/mapreduce/reducer.py . &&         chmod +x mapper.py reducer.py &&         hdfs dfs -rm -r hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05 || true &&         hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar         -files mapper.py,reducer.py         -mapper "python mapper.py"         -reducer "python reducer.py"         -input hdfs:///user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json         -output hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05\'\n        ']
[2025-02-26T20:45:59.595+0000] {subprocess.py:86} INFO - Output:
[2025-02-26T20:46:05.713+0000] {subprocess.py:93} INFO - rm: `hdfs:///user/etudiant/crypto/processed/YYYY=2025/MM=01/DD=05': No such file or directory
[2025-02-26T20:46:14.927+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:14,924 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
[2025-02-26T20:46:15.231+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:15,229 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-02-26T20:46:15.232+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:15,229 INFO impl.MetricsSystemImpl: JobTracker metrics system started
[2025-02-26T20:46:15.307+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:15,305 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
[2025-02-26T20:46:16.997+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:16,995 INFO mapreduce.JobSubmitter: Cleaning up the staging area file:/tmp/hadoop/mapred/staging/root1490045035/.staging/job_local1490045035_0001
[2025-02-26T20:46:17.005+0000] {subprocess.py:93} INFO - 2025-02-26 20:46:17,003 ERROR streaming.StreamJob: Error Launching job : Input path does not exist: hdfs:/user/etudiant/crypto/raw/YYYY=2025/MM=01/DD=05/yfinance_raw.json
[2025-02-26T20:46:17.017+0000] {subprocess.py:93} INFO - Streaming Command Failed!
[2025-02-26T20:46:17.522+0000] {subprocess.py:97} INFO - Command exited with return code 5
[2025-02-26T20:46:17.577+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 5.
[2025-02-26T20:46:17.609+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=yfinance_ingestion_dag, task_id=run_mapreduce, execution_date=20250105T000000, start_date=20250226T204558, end_date=20250226T204617
[2025-02-26T20:46:18.213+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 394 for task run_mapreduce (Bash command failed. The command returned a non-zero exit code 5.; 2754)
[2025-02-26T20:46:18.277+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-02-26T20:46:18.345+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
